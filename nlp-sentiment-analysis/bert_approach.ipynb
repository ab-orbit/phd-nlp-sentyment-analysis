{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pr3ibekrxpe",
   "source": "# Classificação de Sentimentos com BERT\n\nEste notebook implementa um classificador de sentimentos para avaliações de estabelecimentos utilizando **BERT (Bidirectional Encoder Representations from Transformers)**.\n\n## O que é BERT?\n\nBERT é um modelo de linguagem baseado em Transformers desenvolvido pelo Google que revolucionou o NLP:\n\n### Características Principais\n- **Bidirecional**: Analisa o contexto à esquerda E à direita simultaneamente\n- **Pré-treinado**: Treinado em grandes corpus (Wikipedia, BookCorpus)\n- **Transfer Learning**: Fine-tuning para tarefas específicas\n- **Contextual Embeddings**: Cada palavra tem representação diferente dependendo do contexto\n\n### Diferenças das Abordagens Anteriores\n\n| Aspecto | BoW | Word2Vec | BERT |\n|---------|-----|----------|------|\n| Contexto | Nenhum | Janela fixa | Bidirecional completo |\n| Embeddings | Esparsos | Estáticos | Contextuais |\n| Polissemia | Não trata | Não trata | Captura múltiplos sentidos |\n| Pré-treinamento | Não | Sim (Word2Vec) | Sim (massivo) |\n| Arquitetura | N/A | Redes rasas | Transformers (12-24 camadas) |\n\n## Objetivo\nUsar BERT pré-treinado com fine-tuning para classificar avaliações do Yelp como positivas ou negativas, aproveitando a compreensão profunda de linguagem do modelo.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cm8hr7hg9jr",
   "source": "## 1. Importação de Bibliotecas\n\nInstalando e importando as bibliotecas necessárias para trabalhar com BERT usando Hugging Face Transformers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1jbp8n9ov8l",
   "source": "# Instalar bibliotecas necessárias (execute apenas uma vez)\n# !pip install transformers torch datasets accelerate scikit-learn matplotlib seaborn pandas numpy\n\n# NOTA: Se você encontrar erro \"cannot import name 'AdamW' from 'transformers'\",\n# isso é normal nas versões mais recentes. O AdamW foi movido para torch.optim.\n# A célula de imports já está corrigida usando: from torch.optim import AdamW",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T20:38:55.244688Z",
     "start_time": "2026-01-31T20:38:52.038067Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4k8fxpjnkns",
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW  # AdamW agora vem do torch.optim\nfrom transformers import (\n    BertTokenizer, \n    BertForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurações\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Verificar se GPU está disponível\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Usando dispositivo: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    \nprint(\"Bibliotecas importadas com sucesso!\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T20:43:38.036110Z",
     "start_time": "2026-01-31T20:43:37.856743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "rsv01f1fzhb",
   "source": "## 2. Carregamento e Exploração dos Dados\n\nCarregando o dataset do Yelp e preparando para o BERT.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cxd0bqfcjut",
   "source": "# Carregando o dataset do Yelp\ndf = pd.read_csv('dataset/yelp_reviews.csv', names=['label', 'text'])\n\n# Converter labels: 1 (negativo) -> 0, 2 (positivo) -> 1 (formato para BERT)\ndf['label'] = df['label'] - 1\n\nprint(f\"Dimensões do dataset: {df.shape}\")\nprint(f\"\\nDistribuição de classes:\")\nprint(df['label'].value_counts())\nprint(f\"\\n0 = Negativo, 1 = Positivo\")\nprint(f\"\\nPrimeiras linhas:\")\ndf.head()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T20:44:03.820178Z",
     "start_time": "2026-01-31T20:44:03.365070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label                                               text\n",
       "0      1  Contrary to other reviews, I have zero complai...\n",
       "1      0  Last summer I had an appointment to get new ti...\n",
       "2      1  Friendly staff, same starbucks fair you get an...\n",
       "3      0  The food is good. Unfortunately the service is...\n",
       "4      1  Even when we didn't have a car Filene's Baseme..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "photmnjawkg",
   "source": "# Análise do comprimento dos textos (importante para definir max_length do BERT)\ndf['text_length'] = df['text'].apply(lambda x: len(x.split()))\n\nprint(\"Estatísticas do Comprimento dos Textos (em palavras):\")\nprint(df['text_length'].describe())\n\n# Visualizar distribuição\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\nplt.axvline(df['text_length'].quantile(0.95), color='red', linestyle='--', \n            label=f'95º percentil: {df[\"text_length\"].quantile(0.95):.0f}')\nplt.axvline(df['text_length'].median(), color='green', linestyle='--', \n            label=f'Mediana: {df[\"text_length\"].median():.0f}')\nplt.xlabel('Número de Palavras')\nplt.ylabel('Frequência')\nplt.title('Distribuição do Comprimento dos Textos')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.boxplot(df['text_length'])\nplt.ylabel('Número de Palavras')\nplt.title('Boxplot do Comprimento dos Textos')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n95% dos textos têm até {df['text_length'].quantile(0.95):.0f} palavras\")\nprint(f\"Vamos usar max_length=128 tokens para o BERT (cobre a maioria dos casos)\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T20:44:09.893639Z",
     "start_time": "2026-01-31T20:44:09.133512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "95% dos textos têm até 367 palavras\n",
      "Vamos usar max_length=128 tokens para o BERT (cobre a maioria dos casos)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "e98ep7121pp",
   "source": "## 3. Configuração do BERT Tokenizer\n\nO tokenizer do BERT converte texto em tokens que o modelo entende. Principais características:\n- **WordPiece**: Divide palavras em subpalavras quando necessário\n- **Tokens especiais**: [CLS] no início, [SEP] no final\n- **Padding**: Completa sequências curtas para ter o mesmo tamanho\n- **Truncation**: Corta sequências muito longas\n\n**Nota**: Este notebook usa a API moderna do tokenizer (`tokenizer(...)`) ao invés da API antiga deprecada (`tokenizer.encode_plus(...)`). Ambas funcionam de forma idêntica, mas a API moderna é recomendada.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f38oipe5g5",
   "source": "# Carregar tokenizer pré-treinado do BERT\nMODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n\nprint(f\"Tokenizer carregado: {MODEL_NAME}\")\nprint(f\"Tamanho do vocabulário: {tokenizer.vocab_size}\")\n\n# Testar tokenização em um exemplo\nsample_text = df['text'].iloc[0]\nprint(f\"\\nTexto original:\\n{sample_text[:200]}...\")\n\n# Tokenizar (usando API moderna)\nencoded = tokenizer(\n    sample_text,\n    add_special_tokens=True,\n    max_length=128,\n    padding='max_length',\n    truncation=True,\n    return_attention_mask=True,\n    return_tensors='pt'\n)\n\nprint(f\"\\nTokens (primeiros 20):\")\nprint(tokenizer.convert_ids_to_tokens(encoded['input_ids'][0][:20]))\nprint(f\"\\nForma do input_ids: {encoded['input_ids'].shape}\")\nprint(f\"Forma do attention_mask: {encoded['attention_mask'].shape}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T21:14:49.214027Z",
     "start_time": "2026-01-31T21:14:47.057632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer carregado: bert-base-uncased\n",
      "Tamanho do vocabulário: 30522\n",
      "\n",
      "Texto original:\n",
      "Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Bo...\n",
      "\n",
      "Tokens (primeiros 20):\n",
      "['[CLS]', 'contrary', 'to', 'other', 'reviews', ',', 'i', 'have', 'zero', 'complaints', 'about', 'the', 'service', 'or', 'the', 'prices', '.', 'i', 'have', 'been']\n",
      "\n",
      "Forma do input_ids: torch.Size([1, 128])\n",
      "Forma do attention_mask: torch.Size([1, 128])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "6l106pmwcjb",
   "source": "## 4. Criação do Dataset Customizado\n\nCriando uma classe Dataset do PyTorch para alimentar o modelo BERT durante o treinamento.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m4jeb8l53v9",
   "source": "class YelpDataset(Dataset):\n    \"\"\"\n    Dataset customizado para avaliações do Yelp\n    \"\"\"\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        # Tokenizar (usando API moderna - chamada direta ao tokenizer)\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\nprint(\"Classe YelpDataset criada com sucesso!\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T21:15:16.381844Z",
     "start_time": "2026-01-31T21:15:16.243129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe YelpDataset criada com sucesso!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "zf1dyywbfuf",
   "source": "## 5. Divisão dos Dados\n\nDividindo em treino (70%), validação (15%) e teste (15%). A validação é importante para monitorar overfitting durante o treinamento.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ez6a807jjer",
   "source": "# Primeiro: treino (70%) vs temp (30%)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    df['text'].values,\n    df['label'].values,\n    test_size=0.3,\n    random_state=42,\n    stratify=df['label'].values\n)\n\n# Segundo: dividir temp em validação (50%) e teste (50%) = 15% cada\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp,\n    y_temp,\n    test_size=0.5,\n    random_state=42,\n    stratify=y_temp\n)\n\nprint(f\"Tamanho do conjunto de treino: {len(X_train)} amostras\")\nprint(f\"Tamanho do conjunto de validação: {len(X_val)} amostras\")\nprint(f\"Tamanho do conjunto de teste: {len(X_test)} amostras\")\n\n# Criar datasets\ntrain_dataset = YelpDataset(X_train, y_train, tokenizer)\nval_dataset = YelpDataset(X_val, y_val, tokenizer)\ntest_dataset = YelpDataset(X_test, y_test, tokenizer)\n\n# Criar dataloaders\nBATCH_SIZE = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f\"\\nBatch size: {BATCH_SIZE}\")\nprint(f\"Número de batches de treino: {len(train_loader)}\")\nprint(f\"Número de batches de validação: {len(val_loader)}\")\nprint(f\"Número de batches de teste: {len(test_loader)}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T21:15:24.189423Z",
     "start_time": "2026-01-31T21:15:23.710405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: 26600 amostras\n",
      "Tamanho do conjunto de validação: 5700 amostras\n",
      "Tamanho do conjunto de teste: 5700 amostras\n",
      "\n",
      "Batch size: 16\n",
      "Número de batches de treino: 1663\n",
      "Número de batches de validação: 357\n",
      "Número de batches de teste: 357\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "qtu6f8jnjv",
   "source": "## 6. Carregamento do Modelo BERT\n\nCarregando o modelo BERT pré-treinado para classificação de sequências (BertForSequenceClassification).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x87g0e7drea",
   "source": "# Carregar modelo BERT para classificação binária\nmodel = BertForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=2,\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Mover modelo para GPU se disponível\nmodel = model.to(device)\n\nprint(f\"Modelo BERT carregado: {MODEL_NAME}\")\nprint(f\"Número de parâmetros: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Número de parâmetros treináveis: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"Modelo movido para: {device}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T21:16:03.822388Z",
     "start_time": "2026-01-31T21:15:48.505068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo BERT carregado: bert-base-uncased\n",
      "Número de parâmetros: 109,483,778\n",
      "Número de parâmetros treináveis: 109,483,778\n",
      "Modelo movido para: cpu\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "ffofycrvzkt",
   "source": "## 7. Configuração do Treinamento\n\nConfigurando otimizador, learning rate scheduler e funções de treinamento/avaliação.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "56twn3z7sn9",
   "source": "# Hiperparâmetros\nEPOCHS = 3\nLEARNING_RATE = 2e-5\n\n# Otimizador AdamW (versão do Adam com weight decay)\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n\n# Scheduler: decai o learning rate linearmente\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\nprint(f\"Configuração do Treinamento:\")\nprint(f\"  Épocas: {EPOCHS}\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  Total de steps: {total_steps}\")\nprint(f\"  Otimizador: AdamW\")\nprint(f\"  Scheduler: Linear com warmup\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T21:18:37.720742Z",
     "start_time": "2026-01-31T21:18:37.592600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração do Treinamento:\n",
      "  Épocas: 3\n",
      "  Learning Rate: 2e-05\n",
      "  Total de steps: 4989\n",
      "  Otimizador: AdamW\n",
      "  Scheduler: Linear com warmup\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "6a4kjynb6b",
   "source": "# Funções de treinamento e avaliação\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    \"\"\"\n    Treina o modelo por uma época\n    \"\"\"\n    model.train()\n    losses = []\n    correct_predictions = 0\n    \n    progress_bar = tqdm(data_loader, desc='Treinando')\n    \n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        logits = outputs.logits\n        \n        # Calcular acurácia\n        preds = torch.argmax(logits, dim=1)\n        correct_predictions += torch.sum(preds == labels)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        losses.append(loss.item())\n        progress_bar.set_postfix({'loss': loss.item()})\n    \n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\n\ndef eval_model(model, data_loader, device):\n    \"\"\"\n    Avalia o modelo\n    \"\"\"\n    model.eval()\n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc='Avaliando'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            logits = outputs.logits\n            \n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n    \n    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n\nprint(\"Funções de treinamento e avaliação criadas!\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T21:18:47.279577Z",
     "start_time": "2026-01-31T21:18:47.170741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções de treinamento e avaliação criadas!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "3c42xnvvgyr",
   "source": "## 8. Treinamento do Modelo\n\nExecutando o loop de treinamento completo com validação a cada época.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7ce3c0saqdw",
   "source": "# Histórico de métricas\nhistory = {\n    'train_acc': [],\n    'train_loss': [],\n    'val_acc': [],\n    'val_loss': []\n}\n\n# Loop de treinamento\nbest_val_acc = 0\n\nfor epoch in range(EPOCHS):\n    print(f'\\nÉpoca {epoch + 1}/{EPOCHS}')\n    print('-' * 50)\n    \n    # Treinar\n    train_acc, train_loss = train_epoch(\n        model, train_loader, optimizer, scheduler, device\n    )\n    \n    print(f'Train - Loss: {train_loss:.4f}, Acurácia: {train_acc:.4f}')\n    \n    # Validar\n    val_acc, val_loss = eval_model(model, val_loader, device)\n    \n    print(f'Val   - Loss: {val_loss:.4f}, Acurácia: {val_acc:.4f}')\n    \n    # Salvar histórico\n    history['train_acc'].append(train_acc.item())\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc.item())\n    history['val_loss'].append(val_loss)\n    \n    # Salvar melhor modelo\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        print(f'Novo melhor modelo! Acurácia de validação: {val_acc:.4f}')\n\nprint('\\nTreinamento concluído!')",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-31T21:18:54.588455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando:  21%|██        | 347/1663 [22:08<1:12:58,  3.33s/it, loss=0.218]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "my8poj0ej1",
   "source": "# Visualizar curvas de aprendizado\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Loss\naxes[0].plot(history['train_loss'], label='Treino')\naxes[0].plot(history['val_loss'], label='Validação')\naxes[0].set_title('Loss por Época')\naxes[0].set_xlabel('Época')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[0].grid(True)\n\n# Acurácia\naxes[1].plot(history['train_acc'], label='Treino')\naxes[1].plot(history['val_acc'], label='Validação')\naxes[1].set_title('Acurácia por Época')\naxes[1].set_xlabel('Época')\naxes[1].set_ylabel('Acurácia')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "du8u0ixc8k",
   "source": "## 9. Avaliação no Conjunto de Teste\n\nAvaliando o modelo treinado no conjunto de teste com métricas detalhadas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dc7a4o6cf7s",
   "source": "# Função para obter predições\ndef get_predictions(model, data_loader, device):\n    \"\"\"\n    Obtém todas as predições e labels\n    \"\"\"\n    model.eval()\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc='Predizendo'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            \n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    \n    return np.array(predictions), np.array(true_labels)\n\n# Obter predições no conjunto de teste\ny_pred, y_true = get_predictions(model, test_loader, device)\n\n# Calcular métricas\ntest_accuracy = accuracy_score(y_true, y_pred)\nprint(f\"\\nAcurácia no conjunto de teste: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Relatório de classificação\nprint(\"Relatório de Classificação:\")\nprint(classification_report(y_true, y_pred, target_names=['Negativo (0)', 'Positivo (1)']))\n\n# Matriz de confusão\ncm = confusion_matrix(y_true, y_pred)\nprint(\"\\nMatriz de Confusão:\")\nprint(cm)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ydd3xr0d5n",
   "source": "# Visualização da Matriz de Confusão\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n            xticklabels=['Negativo (0)', 'Positivo (1)'],\n            yticklabels=['Negativo (0)', 'Positivo (1)'])\nplt.title('Matriz de Confusão - BERT')\nplt.ylabel('Valor Real')\nplt.xlabel('Valor Predito')\nplt.tight_layout()\nplt.show()\n\n# Métricas detalhadas\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\nVerdadeiros Negativos: {tn}\")\nprint(f\"Falsos Positivos: {fp}\")\nprint(f\"Falsos Negativos: {fn}\")\nprint(f\"Verdadeiros Positivos: {tp}\")\nprint(f\"\\nTaxa de Falso Positivo: {fp/(fp+tn):.4f}\")\nprint(f\"Taxa de Falso Negativo: {fn/(fn+tp):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1kk3tvhk4z",
   "source": "## 10. Teste com Novos Textos\n\nTestando o modelo BERT com exemplos customizados.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4ke2i05p976",
   "source": "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n    \"\"\"\n    Prediz o sentimento de um texto usando o modelo BERT\n    \"\"\"\n    model.eval()\n    \n    # Tokenizar (usando API moderna)\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Predizer\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        prediction = torch.argmax(logits, dim=1).item()\n    \n    sentiment = \"Positivo\" if prediction == 1 else \"Negativo\"\n    return sentiment, prediction\n\n# Testar com exemplos novos\ntest_reviews = [\n    \"This place is amazing! Best food I've ever had. Highly recommend!\",\n    \"Terrible service, cold food, and overpriced. Never coming back.\",\n    \"It was okay, nothing special but not bad either.\",\n    \"Absolutely loved it! The staff was friendly and the atmosphere was great.\",\n    \"Worst experience ever. Waited for an hour and the food was disgusting.\"\n]\n\nprint(\"Predições do BERT para Novos Textos:\")\nprint(\"=\"*70)\nfor i, review in enumerate(test_reviews, 1):\n    sentiment, label = predict_sentiment(review, model, tokenizer, device)\n    print(f\"\\n{i}. Texto: {review}\")\n    print(f\"   Sentimento: {sentiment} (label={label})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mnkrwv26nwa",
   "source": "## 11. Comparação Final: BERT vs SVM (BoW e Embeddings)\n\nComparando as três abordagens implementadas neste projeto.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5x488i8ch8",
   "source": "# Tabela comparativa das três abordagens\ncomparison_data = {\n    'Aspecto': [\n        'Representação',\n        'Dimensionalidade',\n        'Pré-treinamento',\n        'Contexto',\n        'Arquitetura',\n        'Parâmetros Treináveis',\n        'Tempo de Treinamento',\n        'Complexidade',\n        'Hardware Necessário',\n        'Interpretabilidade',\n        'Estado da Arte'\n    ],\n    'SVM + BoW': [\n        'Vetores esparsos',\n        'Alta (5000+)',\n        'Não',\n        'Nenhum',\n        'SVM (linear)',\n        '~5000',\n        'Minutos',\n        'Baixa',\n        'CPU suficiente',\n        'Alta',\n        'Baseline (2000s)'\n    ],\n    'SVM + Embeddings': [\n        'Vetores densos',\n        'Média (100-300)',\n        'Sim (Word2Vec)',\n        'Janela local',\n        'SVM (RBF)',\n        '~100k (Word2Vec)',\n        '< 1 hora',\n        'Média',\n        'CPU suficiente',\n        'Média',\n        'Estado da arte (2013-2017)'\n    ],\n    'BERT': [\n        'Embeddings contextuais',\n        'Alta (768)',\n        'Sim (massivo)',\n        'Bidirecional completo',\n        'Transformers (12 camadas)',\n        '~110M',\n        'Horas (GPU recomendada)',\n        'Alta',\n        'GPU recomendada',\n        'Baixa',\n        'Estado da arte (2018+)'\n    ]\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\n\nprint(\"Comparação das Três Abordagens:\")\nprint(\"=\"*100)\nprint(comparison_df.to_string(index=False))\nprint(\"\\n\" + \"=\"*100)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mmhyb95jm89",
   "source": "## 12. Conclusões\n\n### Resumo do Modelo BERT\n- **Arquitetura**: BERT base (12 camadas Transformer, 110M parâmetros)\n- **Fine-tuning**: 3 épocas com learning rate 2e-5\n- **Dataset**: Avaliações do Yelp com classificação binária\n- **Tokenização**: WordPiece com max_length=128\n\n### Vantagens do BERT\n1. **Compreensão contextual profunda**: Captura nuances de linguagem que BoW e Word2Vec não conseguem\n2. **Embeddings contextuais**: Cada palavra tem representação diferente dependendo do contexto\n3. **Bidirecional**: Analisa o texto completo em ambas as direções\n4. **Transfer learning**: Aproveita conhecimento de bilhões de palavras pré-treinadas\n5. **Estado da arte**: Melhor performance em múltiplas tarefas de NLP\n6. **Polissemia**: Trata palavras com múltiplos significados corretamente\n\n### Desvantagens do BERT\n1. **Custo computacional**: Requer GPU para treinamento eficiente\n2. **Tempo de treinamento**: Muito mais lento que SVM\n3. **Complexidade**: Mais difícil de debugar e entender\n4. **Tamanho do modelo**: 110M parâmetros (~400MB)\n5. **Latência**: Inferência mais lenta que modelos tradicionais\n6. **Overfitting**: Pode overfit em datasets pequenos\n\n### Quando usar cada abordagem?\n\n**Use SVM + BoW quando:**\n- Dataset é grande e vocabulário é rico\n- Recursos computacionais são muito limitados\n- Velocidade é crítica (produção em larga escala)\n- Interpretabilidade é essencial\n- Baseline rápido é necessário\n\n**Use SVM + Embeddings quando:**\n- Quer melhor performance que BoW sem complexidade do BERT\n- Dataset tem palavras raras ou vocabulário limitado\n- Recursos computacionais são limitados (apenas CPU)\n- Precisa de bom equilíbrio entre performance e custo\n\n**Use BERT quando:**\n- Performance é prioridade máxima\n- Dataset tem nuances linguísticas complexas\n- Tem acesso a GPU para treinamento\n- Pode tolerar latência maior na inferência\n- Dataset é suficientemente grande (>1000 exemplos por classe)\n\n### Evolução Histórica\n1. **2000s**: BoW + modelos clássicos (SVM, Naive Bayes)\n2. **2013**: Word2Vec revoluciona embeddings estáticos\n3. **2018**: BERT introduz embeddings contextuais\n4. **Futuro**: Modelos maiores (GPT, LLaMA) e mais eficientes\n\n### Próximos Passos\n1. Experimentar com outros modelos: RoBERTa, DistilBERT, ALBERT\n2. Aumentação de dados para melhorar generalização\n3. Ensemble de múltiplos modelos\n4. Análise de atenção para interpretar decisões do BERT\n5. Quantização e destilação para produção eficiente",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
