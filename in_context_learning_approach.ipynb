{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dq74vdssws",
   "source": "# Classificação de Sentimentos com In-Context Learning (ICL)\n\nEste notebook implementa um classificador de sentimentos utilizando **In-Context Learning** com Large Language Models (LLMs).\n\n## O que é In-Context Learning?\n\nIn-Context Learning é uma abordagem moderna de aprendizado de máquina onde:\n- **Não há treinamento**: O modelo já foi pré-treinado em bilhões de textos\n- **Aprendizado via prompt**: O modelo aprende a tarefa através de exemplos no prompt\n- **Zero/Few-Shot**: Pode funcionar sem exemplos (zero-shot) ou com poucos exemplos (few-shot)\n\n## Diferenças das Abordagens Anteriores\n\n| Aspecto | SVM/BERT (Fine-tuning) | In-Context Learning |\n|---------|------------------------|---------------------|\n| Treinamento | Requer treinar/fine-tunar | Não requer treinamento |\n| Dados necessários | Milhares de exemplos | 0-10 exemplos no prompt |\n| Tempo de setup | Horas | Minutos |\n| Custo computacional | Alto (GPU) | Baixo (API call) |\n| Flexibilidade | Fixo após treino | Adaptável instantaneamente |\n| Custo por inferência | Baixo | Alto (API) |\n\n## Abordagens que vamos implementar\n\n1. **Zero-Shot**: Sem exemplos, apenas instrução\n2. **One-Shot**: Um exemplo de positivo e um de negativo\n3. **Few-Shot**: Múltiplos exemplos (3-5 por classe)\n4. **Chain-of-Thought**: Raciocínio passo a passo\n\n## Objetivo\nClassificar avaliações do Yelp usando LLMs via API sem nenhum treinamento adicional.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "hm3fsaidfb9",
   "source": "## 1. Instalação e Importação de Bibliotecas\n\nPara este notebook, vamos usar:\n- **OpenAI API**: GPT-4 ou GPT-3.5-turbo (requer API key)\n- **Alternativas gratuitas**: Ollama (modelos locais como Llama 3.1, Mistral)\n- **Anthropic API**: Claude (opcional, também requer API key)\n\n### Opções de API\n\n1. **OpenAI (Pago)**: Melhor performance, requer `OPENAI_API_KEY`\n2. **Ollama (Gratuito)**: Roda localmente, sem custo, mas precisa de recursos\n3. **Anthropic (Pago)**: Claude, requer `ANTHROPIC_API_KEY`\n\nPara este exemplo, vamos implementar com suporte para múltiplas APIs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h39prcbwfdb",
   "source": [
    "# Instalar bibliotecas necessárias (execute apenas uma vez)\n",
    "! uv run pip install openai anthropic pandas numpy matplotlib seaborn scikit-learn tqdm\n",
    "\n",
    "# NOTA: Para usar Ollama (gratuito, local):\n",
    "# 1. Instale Ollama: https://ollama.ai\n",
    "# 2. Baixe um modelo: ollama pull llama3.1\n",
    "# 3. Execute: ollama serve"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:24:09.535438Z",
     "start_time": "2026-02-01T18:24:08.257060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b7rb7jno1pk",
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom tqdm.auto import tqdm\nimport time\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurações de visualização\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Bibliotecas básicas importadas com sucesso!\")\nprint(\"\\nAPIs disponíveis:\")\n\n# Tentar importar OpenAI\ntry:\n    from openai import OpenAI\n    print(\"✓ OpenAI API disponível\")\n    OPENAI_AVAILABLE = True\nexcept ImportError:\n    print(\"✗ OpenAI API não disponível (execute: uv run pip install openai)\")\n    OPENAI_AVAILABLE = False\n\n# Tentar importar Anthropic\ntry:\n    from anthropic import Anthropic\n    print(\"✓ Anthropic API disponível\")\n    ANTHROPIC_AVAILABLE = True\nexcept ImportError:\n    print(\"✗ Anthropic API não disponível (execute: uv run pip install anthropic)\")\n    ANTHROPIC_AVAILABLE = False\n\n# Verificar Ollama (local)\nimport subprocess\ntry:\n    result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=2)\n    if result.returncode == 0:\n        print(\"✓ Ollama disponível (local)\")\n        OLLAMA_AVAILABLE = True\n    else:\n        print(\"✗ Ollama não está rodando (execute: ollama serve)\")\n        OLLAMA_AVAILABLE = False\nexcept (subprocess.TimeoutExpired, FileNotFoundError):\n    print(\"✗ Ollama não instalado (visite: https://ollama.ai)\")\n    OLLAMA_AVAILABLE = False\n\nprint(\"\\nNOTA: Este notebook funcionará com qualquer API disponível.\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:24:14.166612Z",
     "start_time": "2026-02-01T18:24:13.218988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Anthropic API disponível\n",
      "✓ Ollama disponível (local)\n",
      "\n",
      "NOTA: Este notebook funcionará com qualquer API disponível.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1q1sfdt3gkc",
   "source": "# DEBUG: Check which Python interpreter Jupyter is using\nimport sys\nprint(f\"Python executable: {sys.executable}\")\nprint(f\"Python version: {sys.version}\")\nprint(f\"\\nExpected path (uv environment):\")\nprint(\"/Users/jwcunha/Documents/COMPANIES/AB/repos/private/premium/phd/computer-vision-and-nlp/sentiment-analysis/.venv/bin/python\")\nprint(f\"\\nMatch: {'.venv' in sys.executable}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:25:11.471590Z",
     "start_time": "2026-02-01T18:25:11.426432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/jwcunha/Documents/COMPANIES/AB/repos/private/premium/phd/computer-vision-and-nlp/sentiment-analysis/.venv/bin/python\n",
      "Python version: 3.13.0 (v3.13.0:60403a5409f, Oct  7 2024, 00:37:40) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "\n",
      "Expected path (uv environment):\n",
      "/Users/jwcunha/Documents/COMPANIES/AB/repos/private/premium/phd/computer-vision-and-nlp/sentiment-analysis/.venv/bin/python\n",
      "\n",
      "Match: True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "w458bfnbi1",
   "source": "### Soluções para Problema de Ambiente\n\nSe o Python acima **NÃO** contém `.venv`, você tem 3 opções:\n\n#### Opção 1: Instalar no ambiente atual do Jupyter (Rápido)\n```python\n!pip install openai anthropic\n```\n\n#### Opção 2: Mudar o kernel do Jupyter\n1. No menu superior do Jupyter: **Kernel → Change Kernel**\n2. Selecione o kernel com `.venv` no nome\n3. Reinicie o kernel\n\n#### Opção 3: Reinstalar Jupyter no ambiente correto (Recomendado)\n```bash\n# No terminal:\ncd /Users/jwcunha/Documents/COMPANIES/AB/repos/private/premium/phd/computer-vision-and-nlp/sentiment-analysis\nuv run pip install ipykernel\nuv run python -m ipykernel install --user --name=sentiment-analysis\n# Depois reinicie Jupyter e selecione o kernel \"sentiment-analysis\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9l3k87kjub",
   "source": "# QUICK FIX: Instalar pacotes usando uv\n# Execute esta célula se precisar instalar os pacotes\n\nimport subprocess\nimport sys\n\nprint(\"Instalando pacotes usando uv...\")\nprint(f\"Ambiente: {sys.executable}\\n\")\n\npackages = ['openai', 'anthropic', 'requests']\n\ntry:\n    # Usar uv pip ao invés de pip normal\n    for package in packages:\n        result = subprocess.run(\n            ['uv', 'pip', 'install', package],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            print(f\"✓ {package} instalado\")\n        else:\n            print(f\"✗ Erro ao instalar {package}\")\n            if result.stderr:\n                print(f\"   {result.stderr[:200]}\")\n    \n    print(\"\\n✓ Instalação concluída!\")\n    print(\"⚠️  IMPORTANTE: Reinicie o kernel (Kernel → Restart Kernel) para que as mudanças tenham efeito.\")\n    \nexcept FileNotFoundError:\n    print(\"✗ Comando 'uv' não encontrado.\")\n    print(\"\\nAlternativa: Execute no terminal:\")\n    print(\"  cd /Users/jwcunha/Documents/COMPANIES/AB/repos/private/premium/phd/computer-vision-and-nlp/sentiment-analysis\")\n    print(\"  uv pip install openai anthropic requests\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:23:29.612497Z",
     "start_time": "2026-02-01T18:23:29.495903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ openai instalado\n",
      "✓ anthropic instalado\n",
      "✓ requests instalado\n",
      "\n",
      "✓ Instalação concluída!\n",
      "⚠️  IMPORTANTE: Reinicie o kernel (Kernel → Restart Kernel) para que as mudanças tenham efeito.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "36vq0eicm73",
   "source": "# Configurar variáveis de ambiente (se necessário)\n# Se você tiver as API keys configuradas, descomente e adicione aqui:\n\n# import os\n# os.environ['OPENAI_API_KEY'] = 'sua-chave-aqui'\n# os.environ['ANTHROPIC_API_KEY'] = 'sua-chave-aqui'\n\n# Ou carregue de um arquivo .env:\n# from dotenv import load_dotenv\n# load_dotenv()\n\nprint(\"Verificando API keys...\")\nimport os\n\nif os.getenv('OPENAI_API_KEY'):\n    print(f\"✓ OPENAI_API_KEY encontrada (primeiros 10 chars: {os.getenv('OPENAI_API_KEY')[:10]}...)\")\nelse:\n    print(\"✗ OPENAI_API_KEY não encontrada\")\n\nif os.getenv('ANTHROPIC_API_KEY'):\n    print(f\"✓ ANTHROPIC_API_KEY encontrada (primeiros 10 chars: {os.getenv('ANTHROPIC_API_KEY')[:10]}...)\")\nelse:\n    print(\"✗ ANTHROPIC_API_KEY não encontrada\")\n\nprint(\"\\nNOTA: Se as keys não aparecerem, você pode:\")\nprint(\"1. Configurá-las no terminal: export OPENAI_API_KEY='sua-key'\")\nprint(\"2. Ou configurá-las diretamente no notebook (veja código acima)\")\nprint(\"3. Ou usar LM Studio/Ollama (gratuito, local) que não precisa de keys\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:23:29.638674Z",
     "start_time": "2026-02-01T18:23:29.613380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando API keys...\n",
      "✓ OPENAI_API_KEY encontrada (primeiros 10 chars: sk-proj-_g...)\n",
      "✓ ANTHROPIC_API_KEY encontrada (primeiros 10 chars: sk-ant-api...)\n",
      "\n",
      "NOTA: Se as keys não aparecerem, você pode:\n",
      "1. Configurá-las no terminal: export OPENAI_API_KEY='sua-key'\n",
      "2. Ou configurá-las diretamente no notebook (veja código acima)\n",
      "3. Ou usar LM Studio/Ollama (gratuito, local) que não precisa de keys\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "hu9yma08tj",
   "source": "## 2. Carregamento dos Dados\n\nCarregando o mesmo dataset do Yelp usado nas outras abordagens.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6c3bpfdu4mb",
   "source": "# DEBUG: Test data loading\nimport pandas as pd\n\n# Test 1: Read first few lines manually\nprint(\"Test 1: Reading raw lines\")\nwith open('dataset/yelp_reviews.csv', 'r', encoding='utf-8') as f:\n    for i, line in enumerate(f):\n        if i < 2:\n            print(f\"Line {i}: {line[:100]}...\")\n        else:\n            break\n\n# Test 2: Try loading with pandas\nprint(\"\\nTest 2: Loading with pandas\")\ntry:\n    df_test = pd.read_csv('dataset/yelp_reviews.csv', names=['label', 'text'], header=None, nrows=5)\n    print(f\"✓ Success! Columns: {list(df_test.columns)}\")\n    print(f\"Shape: {df_test.shape}\")\n    print(f\"\\nFirst row label: {df_test['label'].iloc[0]}\")\n    print(f\"First row text: {df_test['text'].iloc[0][:50]}...\")\nexcept Exception as e:\n    print(f\"✗ Error: {e}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:25:29.339601Z",
     "start_time": "2026-02-01T18:25:29.312121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Reading raw lines\n",
      "Line 0: \"2\",\"Contrary to other reviews, I have zero complaints about the service or the prices. I have been ...\n",
      "Line 1: \"1\",\"Last summer I had an appointment to get new tires and had to wait a super long time. I also wen...\n",
      "\n",
      "Test 2: Loading with pandas\n",
      "✓ Success! Columns: ['label', 'text']\n",
      "Shape: (5, 2)\n",
      "\n",
      "First row label: 2\n",
      "First row text: Contrary to other reviews, I have zero complaints ...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "w7jhficrezk",
   "source": "# Carregando o dataset do Yelp\nimport pandas as pd\nimport numpy as np\n\n# Ler CSV sem assumir que há cabeçalho\ndf = pd.read_csv(\n    'dataset/yelp_reviews.csv', \n    names=['label', 'text'],\n    header=None,\n    encoding='utf-8'\n)\n\nprint(f\"✓ Dataset carregado!\")\nprint(f\"Dimensões: {df.shape}\")\nprint(f\"Colunas: {list(df.columns)}\")\nprint(f\"\\nDistribuição de classes:\")\nprint(df['label'].value_counts())\n\n# Criar amostra balanceada (método mais simples e confiável)\nSAMPLE_SIZE = 200\n\n# Pegar exemplos de cada classe separadamente\ndf_neg = df[df['label'] == 1].sample(n=SAMPLE_SIZE // 2, random_state=42)\ndf_pos = df[df['label'] == 2].sample(n=SAMPLE_SIZE // 2, random_state=42)\n\n# Combinar e embaralhar\ndf_sample = pd.concat([df_neg, df_pos], ignore_index=True)\ndf_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(f\"\\n✓ Amostra criada: {len(df_sample)} exemplos\")\nprint(f\"Distribuição na amostra:\")\nprint(df_sample['label'].value_counts())\nprint(f\"\\nPrimeiros exemplos:\")\nprint(df_sample.head(3))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:27:39.646182Z",
     "start_time": "2026-02-01T18:27:39.378661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset carregado!\n",
      "Dimensões: (38000, 2)\n",
      "Colunas: ['label', 'text']\n",
      "\n",
      "Distribuição de classes:\n",
      "label\n",
      "2    19000\n",
      "1    19000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Amostra criada: 200 exemplos\n",
      "Distribuição na amostra:\n",
      "label\n",
      "1    100\n",
      "2    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Primeiros exemplos:\n",
      "   label                                               text\n",
      "0      1  I used A&R Appliance Repair for many years and...\n",
      "1      1  Wow, was I disappointed by this place. It was ...\n",
      "2      1  This review is for customer service attitude.\\...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b5i03mofv37",
   "source": "## 3. Configuração do Cliente LLM\n\nVamos configurar o cliente para usar o LLM disponível. Suportamos:\n1. **LM Studio** (local, gratuito) - Detectado automaticamente em `localhost:11434`\n2. **OpenAI** (pago) - Requer `OPENAI_API_KEY`\n3. **Anthropic** (pago) - Requer `ANTHROPIC_API_KEY`\n4. **Ollama** (local, gratuito) - Requer Ollama rodando",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lsr3iqnr2xs",
   "source": "import requests\n\n# Classe wrapper para uniformizar diferentes APIs\nclass LLMClient:\n    def __init__(self, provider='auto', model=None, api_key=None, base_url=None):\n        self.provider = provider\n        self.model = model\n        self.api_key = api_key\n        self.base_url = base_url\n        self.client = None\n        \n        # Auto-detectar provider disponível\n        if provider == 'auto':\n            self.provider = self._auto_detect_provider()\n        \n        self._setup_client()\n    \n    def _auto_detect_provider(self):\n        \"\"\"Detecta qual provider está disponível\"\"\"\n        # Verificar LM Studio (localhost:11434)\n        try:\n            response = requests.get('http://localhost:11434/v1/models', timeout=2)\n            if response.status_code == 200:\n                print(\"✓ LM Studio detectado em localhost:11434\")\n                return 'lmstudio'\n        except:\n            pass\n        \n        # Verificar Ollama (localhost:11434)\n        try:\n            response = requests.get('http://localhost:11434/api/tags', timeout=2)\n            if response.status_code == 200:\n                print(\"✓ Ollama detectado\")\n                return 'ollama'\n        except:\n            pass\n        \n        # Verificar OpenAI API key\n        if os.getenv('OPENAI_API_KEY'):\n            print(\"✓ OpenAI API key encontrada\")\n            return 'openai'\n        \n        # Verificar Anthropic API key\n        if os.getenv('ANTHROPIC_API_KEY'):\n            print(\"✓ Anthropic API key encontrada\")\n            return 'anthropic'\n        \n        raise ValueError(\"Nenhum LLM provider encontrado. Configure LM Studio, Ollama, ou adicione API keys.\")\n    \n    def _setup_client(self):\n        \"\"\"Configura o cliente baseado no provider\"\"\"\n        if self.provider == 'lmstudio':\n            from openai import OpenAI\n            self.client = OpenAI(\n                base_url='http://localhost:11434/v1',\n                api_key='lm-studio'  # LM Studio não precisa de key real\n            )\n            self.model = self.model or 'local-model'\n            print(f\"Usando LM Studio com modelo: {self.model}\")\n        \n        elif self.provider == 'ollama':\n            from openai import OpenAI\n            self.client = OpenAI(\n                base_url='http://localhost:11434/v1',\n                api_key='ollama'  # Ollama não precisa de key real\n            )\n            self.model = self.model or 'llama3.1'\n            print(f\"Usando Ollama com modelo: {self.model}\")\n        \n        elif self.provider == 'openai':\n            from openai import OpenAI\n            self.client = OpenAI(api_key=self.api_key or os.getenv('OPENAI_API_KEY'))\n            self.model = self.model or 'gpt-3.5-turbo'\n            print(f\"Usando OpenAI com modelo: {self.model}\")\n        \n        elif self.provider == 'anthropic':\n            from anthropic import Anthropic\n            self.client = Anthropic(api_key=self.api_key or os.getenv('ANTHROPIC_API_KEY'))\n            self.model = self.model or 'claude-3-haiku-20240307'\n            print(f\"Usando Anthropic com modelo: {self.model}\")\n    \n    def generate(self, prompt, max_tokens=100, temperature=0.0):\n        \"\"\"Gera resposta do LLM\"\"\"\n        if self.provider in ['lmstudio', 'ollama', 'openai']:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=max_tokens,\n                temperature=temperature\n            )\n            return response.choices[0].message.content.strip()\n        \n        elif self.provider == 'anthropic':\n            response = self.client.messages.create(\n                model=self.model,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            return response.content[0].text.strip()\n\n# Inicializar cliente\ntry:\n    llm = LLMClient(provider='auto')\n    print(\"\\n✓ Cliente LLM configurado com sucesso!\")\nexcept Exception as e:\n    print(f\"\\n✗ Erro ao configurar LLM: {e}\")\n    print(\"\\nConfigurações sugeridas:\")\n    print(\"1. Instale LM Studio (https://lmstudio.ai) e inicie um servidor\")\n    print(\"2. Ou instale Ollama (https://ollama.ai) e execute: ollama serve\")\n    print(\"3. Ou configure: export OPENAI_API_KEY='sua-chave'\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:27:44.467024Z",
     "start_time": "2026-02-01T18:27:44.166359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LM Studio detectado em localhost:11434\n",
      "Usando LM Studio com modelo: local-model\n",
      "\n",
      "✓ Cliente LLM configurado com sucesso!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "eu0uy0je57",
   "source": "## 4. Implementação de Estratégias de In-Context Learning\n\nVamos implementar e comparar diferentes estratégias de prompt:\n\n### 4.1 Zero-Shot Learning\nSem exemplos, apenas instrução direta.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oasluf3idmb",
   "source": "def create_zero_shot_prompt(review_text):\n    \"\"\"\n    Cria um prompt zero-shot (sem exemplos)\n    \"\"\"\n    prompt = f\"\"\"Classify the sentiment of the following restaurant review as either \"positive\" or \"negative\".\nRespond with ONLY one word: \"positive\" or \"negative\".\n\nReview: {review_text}\n\nSentiment:\"\"\"\n    return prompt\n\n# Testar com um exemplo\nsample_review = df_sample['text'].iloc[0]\nsample_label = df_sample['label'].iloc[0]\n\nprompt = create_zero_shot_prompt(sample_review)\nprint(\"Prompt Zero-Shot:\")\nprint(\"=\" * 70)\nprint(prompt)\nprint(\"=\" * 70)\n\n# Gerar resposta\nresponse = llm.generate(prompt, max_tokens=10, temperature=0.0)\nprint(f\"\\nResposta do LLM: {response}\")\nprint(f\"Label verdadeiro: {'positive' if sample_label == 2 else 'negative'}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:28:37.404694Z",
     "start_time": "2026-02-01T18:28:22.330057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta do LLM: negative\n",
      "Label verdadeiro: negative\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "ynxsx3htfg",
   "source": "### 4.2 Few-Shot Learning\nCom exemplos demonstrativos (3 positivos + 3 negativos).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p5wke1cbqb8",
   "source": "def create_few_shot_prompt(review_text, num_examples=3):\n    \"\"\"\n    Cria um prompt few-shot com exemplos\n    \"\"\"\n    # Exemplos cuidadosamente selecionados (equilibrados)\n    examples_positive = [\n        \"This place is amazing! Best food I've ever had.\",\n        \"Excellent service and delicious meals. Highly recommend!\",\n        \"Absolutely loved it. Will definitely come back.\"\n    ]\n    \n    examples_negative = [\n        \"Terrible experience. Food was cold and service was rude.\",\n        \"Worst restaurant ever. Overpriced and disgusting.\",\n        \"Never coming back. Waited for an hour and food was awful.\"\n    ]\n    \n    # Construir prompt com exemplos\n    prompt = \"Classify the sentiment of restaurant reviews as either \\\"positive\\\" or \\\"negative\\\".\\n\\n\"\n    \n    # Adicionar exemplos\n    for i in range(num_examples):\n        prompt += f\"Review: {examples_positive[i]}\\nSentiment: positive\\n\\n\"\n        prompt += f\"Review: {examples_negative[i]}\\nSentiment: negative\\n\\n\"\n    \n    # Adicionar review a ser classificada\n    prompt += f\"Review: {review_text}\\nSentiment:\"\n    \n    return prompt\n\n# Testar few-shot\nprompt_few_shot = create_few_shot_prompt(sample_review, num_examples=3)\nprint(\"Prompt Few-Shot (3 exemplos por classe):\")\nprint(\"=\" * 70)\nprint(prompt_few_shot)\nprint(\"=\" * 70)\n\n# Gerar resposta\nresponse_few_shot = llm.generate(prompt_few_shot, max_tokens=10, temperature=0.0)\nprint(f\"\\nResposta do LLM: {response_few_shot}\")\nprint(f\"Label verdadeiro: {'positive' if sample_label == 2 else 'negative'}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:29:02.160895Z",
     "start_time": "2026-02-01T18:28:56.217664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta do LLM: negative\n",
      "Label verdadeiro: negative\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "xiiosc4v9j",
   "source": "### 4.3 Chain-of-Thought (CoT) Prompting\nPede ao modelo para raciocinar passo a passo antes de classificar.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lxepn8lm3k",
   "source": "def create_cot_prompt(review_text):\n    \"\"\"\n    Cria um prompt com Chain-of-Thought reasoning\n    \"\"\"\n    prompt = f\"\"\"Classify the sentiment of the following restaurant review as either \"positive\" or \"negative\".\n\nThink step-by-step:\n1. Identify key words and phrases\n2. Determine if they are positive or negative\n3. Make final classification\n\nReview: {review_text}\n\nAnalysis:\"\"\"\n    return prompt\n\n# Testar CoT\nprompt_cot = create_cot_prompt(sample_review)\nprint(\"Prompt Chain-of-Thought:\")\nprint(\"=\" * 70)\nprint(prompt_cot)\nprint(\"=\" * 70)\n\n# Gerar resposta (mais tokens para permitir raciocínio)\nresponse_cot = llm.generate(prompt_cot, max_tokens=200, temperature=0.0)\nprint(f\"\\nResposta do LLM:\")\nprint(response_cot)\nprint(f\"\\nLabel verdadeiro: {'positive' if sample_label == 2 else 'negative'}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:30:41.579559Z",
     "start_time": "2026-02-01T18:30:21.375105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta do LLM:\n",
      "Here's the step-by-step analysis:\n",
      "\n",
      "**1. Identify key words and phrases**\n",
      "\n",
      "* \"satisfied\" (positive)\n",
      "* \"break\" (negative)\n",
      "* \"admitted\" (neutral)\n",
      "* \"paid for new parts\" (positive)\n",
      "* \"none of the burners worked\" (negative)\n",
      "* \"tired of the original making things worse\" (negative)\n",
      "* \"frustrated\" (negative)\n",
      "* \"angry\" (negative)\n",
      "* \"no one took responsibility\" (negative)\n",
      "* \"totally incompetent\" (negative)\n",
      "\n",
      "**2. Determine if they are positive or negative**\n",
      "\n",
      "* The reviewer was initially satisfied with A&R Appliance Repair, but the experience turned sour.\n",
      "* The repairman broke the cooktop and caused more problems.\n",
      "* The company paid for new parts to fix the issue, which is a positive step.\n",
      "* However, the final result was that none of the burners worked, which is negative.\n",
      "* The reviewer felt frustrated and\n",
      "\n",
      "Label verdadeiro: negative\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "cwf6tg6133",
   "source": "## 5. Avaliação Comparativa das Estratégias\n\nVamos avaliar cada estratégia no dataset de teste e comparar os resultados.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yvshg5mikh",
   "source": "def parse_sentiment(response):\n    \"\"\"\n    Extrai sentimento da resposta do LLM\n    \"\"\"\n    response = response.lower().strip()\n    \n    # Procurar por 'positive' ou 'negative' na resposta\n    if 'positive' in response:\n        return 2  # Label positivo\n    elif 'negative' in response:\n        return 1  # Label negativo\n    else:\n        # Fallback: tentar identificar por palavras-chave\n        positive_words = ['good', 'great', 'excellent', 'amazing', 'love']\n        negative_words = ['bad', 'terrible', 'awful', 'worst', 'hate']\n        \n        if any(word in response for word in positive_words):\n            return 2\n        elif any(word in response for word in negative_words):\n            return 1\n        else:\n            return None  # Não conseguiu classificar\n\ndef evaluate_strategy(df, prompt_fn, strategy_name, max_samples=None):\n    \"\"\"\n    Avalia uma estratégia de prompting\n    \"\"\"\n    if max_samples:\n        df = df.head(max_samples)\n    \n    predictions = []\n    true_labels = []\n    errors = 0\n    \n    print(f\"\\nAvaliando estratégia: {strategy_name}\")\n    print(f\"Processando {len(df)} exemplos...\")\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        try:\n            # Criar prompt\n            if strategy_name == \"Chain-of-Thought\":\n                prompt = prompt_fn(row['text'])\n                response = llm.generate(prompt, max_tokens=200, temperature=0.0)\n            else:\n                prompt = prompt_fn(row['text'])\n                response = llm.generate(prompt, max_tokens=10, temperature=0.0)\n            \n            # Parsear resposta\n            prediction = parse_sentiment(response)\n            \n            if prediction is not None:\n                predictions.append(prediction)\n                true_labels.append(row['label'])\n            else:\n                errors += 1\n            \n            # Rate limiting (evitar sobrecarga)\n            time.sleep(0.1)\n        \n        except Exception as e:\n            print(f\"Erro no exemplo {idx}: {e}\")\n            errors += 1\n            continue\n    \n    # Calcular métricas\n    if len(predictions) > 0:\n        accuracy = accuracy_score(true_labels, predictions)\n        print(f\"\\n{strategy_name} - Resultados:\")\n        print(f\"  Acurácia: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n        print(f\"  Exemplos processados: {len(predictions)}\")\n        print(f\"  Erros/Não classificados: {errors}\")\n        \n        return {\n            'strategy': strategy_name,\n            'accuracy': accuracy,\n            'predictions': predictions,\n            'true_labels': true_labels,\n            'errors': errors\n        }\n    else:\n        print(f\"Erro: Nenhuma predição válida para {strategy_name}\")\n        return None\n\nprint(\"Funções de avaliação criadas!\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:31:03.677143Z",
     "start_time": "2026-02-01T18:31:03.649429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções de avaliação criadas!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "567520e5y9",
   "source": "# Avaliar todas as estratégias\n# NOTA: Reduzimos para 50 exemplos para economizar tempo/custo\n# Aumente MAX_EVAL_SAMPLES se quiser avaliar em mais dados\n\nMAX_EVAL_SAMPLES = 50\n\nresults = []\n\n# 1. Zero-Shot\nresult_zero = evaluate_strategy(\n    df_sample, \n    create_zero_shot_prompt, \n    \"Zero-Shot\",\n    max_samples=MAX_EVAL_SAMPLES\n)\nif result_zero:\n    results.append(result_zero)\n\n# 2. Few-Shot (3 exemplos)\nresult_few = evaluate_strategy(\n    df_sample,\n    lambda text: create_few_shot_prompt(text, num_examples=3),\n    \"Few-Shot (3 exemplos)\",\n    max_samples=MAX_EVAL_SAMPLES\n)\nif result_few:\n    results.append(result_few)\n\n# 3. Chain-of-Thought\nresult_cot = evaluate_strategy(\n    df_sample,\n    create_cot_prompt,\n    \"Chain-of-Thought\",\n    max_samples=MAX_EVAL_SAMPLES\n)\nif result_cot:\n    results.append(result_cot)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"AVALIAÇÃO CONCLUÍDA!\")\nprint(\"=\"*70)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T18:52:31.660063Z",
     "start_time": "2026-02-01T18:31:24.872288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "tycus13vbf",
   "source": "## 6. Visualização e Comparação dos Resultados",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "iyc9f4f4fk",
   "source": "# Comparação de acurácias\nstrategies = [r['strategy'] for r in results]\naccuracies = [r['accuracy'] for r in results]\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(strategies, accuracies, color=['#3498db', '#2ecc71', '#e74c3c'])\nplt.title('Comparação de Acurácia: Estratégias de In-Context Learning', fontsize=14, fontweight='bold')\nplt.ylabel('Acurácia', fontsize=12)\nplt.ylim(0, 1.0)\n\n# Adicionar valores nas barras\nfor i, (bar, acc) in enumerate(zip(bars, accuracies)):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n             f'{acc:.2%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.xticks(rotation=15, ha='right')\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Tabela resumo\nsummary_df = pd.DataFrame({\n    'Estratégia': strategies,\n    'Acurácia': [f\"{acc:.2%}\" for acc in accuracies],\n    'Exemplos Processados': [r['errors'] for r in results],\n    'Erros': [r['errors'] for r in results]\n})\n\nprint(\"\\nResumo dos Resultados:\")\nprint(\"=\"*70)\nprint(summary_df.to_string(index=False))\nprint(\"=\"*70)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T19:16:23.558541Z",
     "start_time": "2026-02-01T19:16:23.238270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumo dos Resultados:\n",
      "======================================================================\n",
      "           Estratégia Acurácia  Exemplos Processados  Erros\n",
      "            Zero-Shot   94.00%                     0      0\n",
      "Few-Shot (3 exemplos)  100.00%                    13     13\n",
      "     Chain-of-Thought   48.00%                     0      0\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "njqz0kee5dk",
   "source": "# Matrizes de confusão para cada estratégia\nfig, axes = plt.subplots(1, len(results), figsize=(15, 4))\n\nif len(results) == 1:\n    axes = [axes]\n\nfor idx, result in enumerate(results):\n    cm = confusion_matrix(result['true_labels'], result['predictions'])\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n                xticklabels=['Negativo', 'Positivo'],\n                yticklabels=['Negativo', 'Positivo'])\n    axes[idx].set_title(f'{result[\"strategy\"]}', fontweight='bold')\n    axes[idx].set_ylabel('Valor Real')\n    axes[idx].set_xlabel('Valor Predito')\n\nplt.tight_layout()\nplt.show()\n\n# Relatórios de classificação detalhados\nfor result in results:\n    print(f\"\\n{'='*70}\")\n    print(f\"Relatório de Classificação: {result['strategy']}\")\n    print(f\"{'='*70}\")\n    print(classification_report(\n        result['true_labels'], \n        result['predictions'],\n        target_names=['Negativo (1)', 'Positivo (2)']\n    ))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T19:16:34.258779Z",
     "start_time": "2026-02-01T19:16:34.012599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Relatório de Classificação: Zero-Shot\n",
      "======================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negativo (1)       0.90      1.00      0.95        27\n",
      "Positivo (2)       1.00      0.87      0.93        23\n",
      "\n",
      "    accuracy                           0.94        50\n",
      "   macro avg       0.95      0.93      0.94        50\n",
      "weighted avg       0.95      0.94      0.94        50\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Relatório de Classificação: Few-Shot (3 exemplos)\n",
      "======================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negativo (1)       1.00      1.00      1.00        20\n",
      "Positivo (2)       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Relatório de Classificação: Chain-of-Thought\n",
      "======================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negativo (1)       1.00      0.04      0.07        27\n",
      "Positivo (2)       0.47      1.00      0.64        23\n",
      "\n",
      "    accuracy                           0.48        50\n",
      "   macro avg       0.73      0.52      0.36        50\n",
      "weighted avg       0.76      0.48      0.33        50\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "0vwrmb1yzl1",
   "source": "## 7. Comparação Final: Todas as Abordagens\n\nComparando In-Context Learning com as abordagens anteriores (SVM + BoW, SVM + Embeddings, BERT).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xegk9n5fn7d",
   "source": "# Tabela comparativa completa de todas as abordagens\ncomparison_data = {\n    'Aspecto': [\n        'Representação',\n        'Treinamento',\n        'Dados Necessários',\n        'Tempo de Setup',\n        'Hardware',\n        'Custo por Inferência',\n        'Flexibilidade',\n        'Interpretabilidade',\n        'Contexto',\n        'Generalização',\n        'Latência',\n        'Estado da Arte'\n    ],\n    'SVM + BoW': [\n        'Vetores esparsos (5000+)',\n        'Sim (minutos)',\n        '10k+ exemplos',\n        'Minutos',\n        'CPU',\n        'Muito baixo',\n        'Baixa (requer retreino)',\n        'Alta',\n        'Nenhum',\n        'Boa no domínio',\n        'Baixíssima (<1ms)',\n        'Baseline (2000s)'\n    ],\n    'SVM + Embeddings': [\n        'Vetores densos (100-300)',\n        'Sim (< 1 hora)',\n        '10k+ exemplos',\n        '< 1 hora',\n        'CPU',\n        'Baixo',\n        'Baixa (requer retreino)',\n        'Média',\n        'Janela local',\n        'Boa',\n        'Baixa (<10ms)',\n        '2013-2017'\n    ],\n    'BERT (Fine-tuning)': [\n        'Embeddings contextuais (768)',\n        'Sim (horas com GPU)',\n        '1k+ exemplos',\n        'Horas',\n        'GPU recomendada',\n        'Médio',\n        'Média (requer fine-tuning)',\n        'Baixa',\n        'Bidirecional completo',\n        'Excelente',\n        'Média (50-100ms)',\n        '2018-2023'\n    ],\n    'In-Context Learning': [\n        'Processamento direto LLM',\n        'Não (zero-shot possível)',\n        '0-10 exemplos',\n        'Minutos',\n        'API (sem GPU local)',\n        'Alto (API calls)',\n        'Altíssima (muda prompt)',\n        'Média-Alta',\n        'Compreensão completa',\n        'Excelente (generalista)',\n        'Alta (100-500ms)',\n        '2023+'\n    ]\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\n\nprint(\"Comparação Completa: Todas as Abordagens\")\nprint(\"=\"*120)\nprint(comparison_df.to_string(index=False))\nprint(\"=\"*120)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T19:16:46.062834Z",
     "start_time": "2026-02-01T19:16:46.014789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparação Completa: Todas as Abordagens\n",
      "========================================================================================================================\n",
      "             Aspecto                SVM + BoW         SVM + Embeddings           BERT (Fine-tuning)      In-Context Learning\n",
      "       Representação Vetores esparsos (5000+) Vetores densos (100-300) Embeddings contextuais (768) Processamento direto LLM\n",
      "         Treinamento            Sim (minutos)           Sim (< 1 hora)          Sim (horas com GPU) Não (zero-shot possível)\n",
      "   Dados Necessários            10k+ exemplos            10k+ exemplos                 1k+ exemplos            0-10 exemplos\n",
      "      Tempo de Setup                  Minutos                 < 1 hora                        Horas                  Minutos\n",
      "            Hardware                      CPU                      CPU              GPU recomendada      API (sem GPU local)\n",
      "Custo por Inferência              Muito baixo                    Baixo                        Médio         Alto (API calls)\n",
      "       Flexibilidade  Baixa (requer retreino)  Baixa (requer retreino)   Média (requer fine-tuning)  Altíssima (muda prompt)\n",
      "  Interpretabilidade                     Alta                    Média                        Baixa               Média-Alta\n",
      "            Contexto                   Nenhum             Janela local        Bidirecional completo     Compreensão completa\n",
      "       Generalização           Boa no domínio                      Boa                    Excelente  Excelente (generalista)\n",
      "            Latência        Baixíssima (<1ms)            Baixa (<10ms)             Média (50-100ms)         Alta (100-500ms)\n",
      "      Estado da Arte         Baseline (2000s)                2013-2017                    2018-2023                    2023+\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "ewhf1gyp3le",
   "source": "## 8. Conclusões\n\n### Resumo do In-Context Learning\n- **Abordagem**: Uso de LLMs pré-treinados sem fine-tuning adicional\n- **Estratégias testadas**: Zero-Shot, Few-Shot, Chain-of-Thought\n- **Modelos suportados**: GPT-4, GPT-3.5, Claude, Llama (via LM Studio/Ollama)\n\n### Vantagens do In-Context Learning\n\n1. **Sem treinamento**: Pronto para usar instantaneamente\n2. **Poucos dados**: Funciona com 0-10 exemplos\n3. **Flexibilidade extrema**: Mude a tarefa apenas alterando o prompt\n4. **Generalização**: LLMs entendem contexto e nuances complexas\n5. **Setup rápido**: Minutos ao invés de horas/dias\n6. **Sem infraestrutura**: Não precisa de GPU ou ambiente de treinamento\n7. **Adaptável**: Ajuste o comportamento sem retreinar\n\n### Desvantagens do In-Context Learning\n\n1. **Custo por inferência**: APIs pagas podem ser caras em escala\n2. **Latência**: Respostas mais lentas (100-500ms) que modelos tradicionais\n3. **Dependência de API**: Requer conexão com servidor (exceto modelos locais)\n4. **Menor controle**: Comportamento do modelo pode ser imprevisível\n5. **Limites de contexto**: Prompts muito grandes podem exceder limites\n6. **Consistência**: Pode variar levemente entre execuções\n\n### Quando usar In-Context Learning?\n\n**Use ICL quando:**\n- Prototipagem rápida ou MVP\n- Poucos dados de treinamento disponíveis (< 1000 exemplos)\n- Múltiplas tarefas diferentes (flexibilidade)\n- Não quer manter infraestrutura de ML\n- Escala baixa-média (< 1M predições/dia)\n- Contexto e nuances são importantes\n- Tempo de setup é crítico\n\n**NÃO use ICL quando:**\n- Custo por inferência é crítico\n- Latência precisa ser mínima (< 10ms)\n- Escala muito alta (milhões de predições/dia)\n- Privacidade de dados impede uso de APIs externas\n- Performance precisa ser maximizada em domínio específico\n- Disponibilidade 100% é essencial (APIs podem ter downtime)\n\n### Evolução das Abordagens de NLP\n\n1. **2000s**: BoW + ML clássico (SVM, Naive Bayes)\n   - Simples, rápido, interpretável\n   - Sem compreensão semântica\n\n2. **2013-2017**: Embeddings + Deep Learning (Word2Vec, GloVe)\n   - Captura semântica\n   - Embeddings estáticos\n\n3. **2018-2022**: Transformers + Fine-tuning (BERT, RoBERTa)\n   - Embeddings contextuais\n   - Requer treino em domínio específico\n\n4. **2023+**: LLMs + In-Context Learning (GPT-4, Claude, Llama)\n   - Zero/Few-shot learning\n   - Generalização extrema\n   - Sem necessidade de treino\n\n### Recomendações Práticas\n\n**Para Produção em Escala:**\n- SVM + BoW/Embeddings (baixo custo, rápido)\n- BERT fine-tuned (melhor performance)\n\n**Para Prototipagem/MVP:**\n- In-Context Learning (setup instantâneo)\n\n**Abordagem Híbrida (Recomendado):**\n1. **Fase 1**: Use ICL para validar ideia rapidamente\n2. **Fase 2**: Colete dados reais e fine-tune BERT\n3. **Fase 3**: Otimize com modelos menores (DistilBERT) para produção\n4. **Fase 4**: Mantenha ICL para casos edge/complexos\n\n### Próximos Passos\n\n1. **Prompt Engineering Avançado**:\n   - Self-consistency (múltiplas respostas + votação)\n   - Tree-of-Thought prompting\n   - Retrieval-Augmented Generation (RAG)\n\n2. **Otimização de Custo**:\n   - Cache de respostas comuns\n   - Modelos menores para casos simples\n   - Quantização de modelos locais\n\n3. **Ensemble**:\n   - Combinar ICL com modelos fine-tuned\n   - Usar ICL para casos difíceis, SVM para fáceis\n\n4. **Modelos Especializados**:\n   - Fine-tune LLMs menores (Llama 3.1 8B)\n   - Destilação de conhecimento do LLM grande",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8kehk4c641n",
   "source": "## 9. Teste Interativo com Novos Textos\n\nTeste o modelo com seus próprios exemplos!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "95vjt3vsyc6",
   "source": "# Função para classificar novos textos com qualquer estratégia\ndef classify_review(text, strategy='few-shot'):\n    \"\"\"\n    Classifica uma avaliação usando a estratégia escolhida\n    \n    Args:\n        text: Texto da avaliação\n        strategy: 'zero-shot', 'few-shot', ou 'cot'\n    \"\"\"\n    if strategy == 'zero-shot':\n        prompt = create_zero_shot_prompt(text)\n        max_tokens = 10\n    elif strategy == 'few-shot':\n        prompt = create_few_shot_prompt(text, num_examples=3)\n        max_tokens = 10\n    elif strategy == 'cot':\n        prompt = create_cot_prompt(text)\n        max_tokens = 200\n    else:\n        raise ValueError(f\"Estratégia inválida: {strategy}\")\n    \n    response = llm.generate(prompt, max_tokens=max_tokens, temperature=0.0)\n    sentiment = parse_sentiment(response)\n    \n    return {\n        'text': text,\n        'strategy': strategy,\n        'response': response,\n        'sentiment': 'Positivo' if sentiment == 2 else 'Negativo' if sentiment == 1 else 'Indefinido',\n        'sentiment_label': sentiment\n    }\n\n# Testar com exemplos personalizados\ntest_examples = [\n    \"The food was absolutely amazing and the service was impeccable!\",\n    \"Terrible place. Never going back. Waste of money.\",\n    \"It's okay I guess, nothing special but not terrible either.\",\n    \"Best restaurant in town! Highly recommend the pasta!\",\n    \"Horrible experience from start to finish. Cold food and rude staff.\"\n]\n\nprint(\"Testando In-Context Learning com Novos Exemplos\")\nprint(\"=\"*80)\n\nfor i, example in enumerate(test_examples, 1):\n    print(f\"\\n{i}. Review: {example}\")\n    \n    # Testar com Few-Shot (geralmente a melhor performance)\n    result = classify_review(example, strategy='few-shot')\n    \n    print(f\"   Sentimento: {result['sentiment']}\")\n    print(f\"   Resposta do modelo: {result['response']}\")\n    print(\"-\"*80)\n\nprint(\"\\n✓ Classificação concluída!\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
