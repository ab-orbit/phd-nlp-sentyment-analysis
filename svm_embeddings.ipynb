{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aktg9a0019s",
   "metadata": {},
   "source": [
    "# Classificação de Sentimentos com SVM + Word Embeddings\n",
    "\n",
    "Este notebook implementa um classificador de sentimentos para avaliações de estabelecimentos utilizando:\n",
    "- **SVM (Support Vector Machine)**: algoritmo de aprendizado supervisionado para classificação\n",
    "- **Word Embeddings**: representação vetorial densa de palavras que captura relações semânticas\n",
    "\n",
    "## Diferença entre BoW e Embeddings\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "- Vetores esparsos de alta dimensionalidade\n",
    "- Cada palavra é independente (não captura relações semânticas)\n",
    "- Baseado em frequência de palavras\n",
    "\n",
    "### Word Embeddings\n",
    "- Vetores densos de menor dimensionalidade (tipicamente 50-300 dimensões)\n",
    "- Palavras similares têm vetores próximos no espaço vetorial\n",
    "- Captura relações semânticas e sintáticas\n",
    "- Exemplos: Word2Vec, GloVe, FastText\n",
    "\n",
    "## Objetivo\n",
    "Classificar avaliações de estabelecimentos como positivas ou negativas usando embeddings pré-treinados ou treinados no corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39y45hkis69",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas\n",
    "\n",
    "Importando as bibliotecas necessárias para processamento de texto, embeddings, modelagem e avaliação."
   ]
  },
  {
   "cell_type": "code",
   "id": "7694ffce2e5a73de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:38:30.854478Z",
     "iopub.status.busy": "2026-02-01T10:38:30.854113Z",
     "iopub.status.idle": "2026-02-01T10:38:32.393206Z",
     "shell.execute_reply": "2026-02-01T10:38:32.392661Z",
     "shell.execute_reply.started": "2026-02-01T10:38:30.854446Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:01:17.282379Z",
     "start_time": "2026-02-01T12:01:17.250177Z"
    }
   },
   "source": [
    "# executar uma única vez\n",
    "# ! uv run sync\n",
    "# ! uv run pip install gensim"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## troubleshooting with jupyter environment\n",
    "Solution: Launch Jupyter through the uv environment\n",
    "\n",
    "  Option 1: Restart Jupyter with uv (Recommended)\n",
    "\n",
    "  # Stop your current Jupyter server, then run:\n",
    "  uv run jupyter lab\n",
    "\n",
    "  This ensures Jupyter uses the Python environment where gensim is installed.\n",
    "\n",
    "  Option 2: If Jupyter is already running\n",
    "\n",
    "  1. In your notebook, click on the kernel name in the top-right corner\n",
    "  2. Select \"Python 3 (ipykernel)\" from the .venv environment\n",
    "  3. Restart the kernel\n",
    "\n",
    "  Option 3: Quick verification in your notebook\n",
    "  Add this cell to check which Python your notebook is using:\n",
    "\n",
    "```python\n",
    "  import sys\n",
    "  print(sys.executable)\n",
    "  print(sys.path)\n",
    "```\n",
    "  It should show: /Users/jwcunha/Documents/COMPANIES/AB/repos/private/premium/phd/computer-vision-and-nlp/sentiment-analysis/.venv/bin/python"
   ],
   "id": "2b63016cdc1c8232"
  },
  {
   "cell_type": "code",
   "id": "gnyxtjhrfcg",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:38:34.278372Z",
     "iopub.status.busy": "2026-02-01T10:38:34.277651Z",
     "iopub.status.idle": "2026-02-01T10:39:28.636895Z",
     "shell.execute_reply": "2026-02-01T10:39:28.636329Z",
     "shell.execute_reply.started": "2026-02-01T10:38:34.278334Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:01:17.676986Z",
     "start_time": "2026-02-01T12:01:17.284772Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de visualização\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "um9s01q21nf",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados\n",
    "\n",
    "Carregando o dataset de avaliações do Yelp para análise e treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "id": "751fj1j287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:39:35.812740Z",
     "iopub.status.busy": "2026-02-01T10:39:35.811686Z",
     "iopub.status.idle": "2026-02-01T10:39:36.083164Z",
     "shell.execute_reply": "2026-02-01T10:39:36.082508Z",
     "shell.execute_reply.started": "2026-02-01T10:39:35.812700Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:01:17.897824Z",
     "start_time": "2026-02-01T12:01:17.683492Z"
    }
   },
   "source": [
    "# Carregando o dataset do Yelp\n",
    "df = pd.read_csv('dataset/yelp_reviews.csv', names=['label', 'text'])\n",
    "\n",
    "print(f\"Dimensões do dataset: {df.shape}\")\n",
    "print(f\"\\nDistribuição de classes:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label                                               text\n",
       "0      2  Contrary to other reviews, I have zero complai...\n",
       "1      1  Last summer I had an appointment to get new ti...\n",
       "2      2  Friendly staff, same starbucks fair you get an...\n",
       "3      1  The food is good. Unfortunately the service is...\n",
       "4      2  Even when we didn't have a car Filene's Baseme..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "tef2g5esdm9",
   "metadata": {},
   "source": [
    "## 3. Exploração Inicial dos Dados\n",
    "\n",
    "Verificando a qualidade dos dados e identificando possíveis problemas."
   ]
  },
  {
   "cell_type": "code",
   "id": "qn192y7pd4k",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:39:38.486286Z",
     "iopub.status.busy": "2026-02-01T10:39:38.485883Z",
     "iopub.status.idle": "2026-02-01T10:39:38.711791Z",
     "shell.execute_reply": "2026-02-01T10:39:38.711239Z",
     "shell.execute_reply.started": "2026-02-01T10:39:38.486258Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:01:18.115932Z",
     "start_time": "2026-02-01T12:01:17.924952Z"
    }
   },
   "source": [
    "# Verificar valores nulos\n",
    "print(\"Valores Nulos:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Informações sobre o dataset\n",
    "print(\"Informações do Dataset:\")\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Estatísticas do comprimento dos textos\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Estatísticas de Comprimento:\")\n",
    "print(f\"Caracteres - Média: {df['text_length'].mean():.0f}, Mediana: {df['text_length'].median():.0f}\")\n",
    "print(f\"Palavras - Média: {df['word_count'].mean():.0f}, Mediana: {df['word_count'].median():.0f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatísticas de Comprimento:\n",
      "Caracteres - Média: 723, Mediana: 527\n",
      "Palavras - Média: 133, Mediana: 97\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "tbvopcjsbkd",
   "metadata": {},
   "source": [
    "## 4. Pré-processamento de Texto\n",
    "\n",
    "Para trabalhar com embeddings, o pré-processamento precisa manter a estrutura das palavras. Vamos:\n",
    "- Converter para minúsculas\n",
    "- Remover pontuação\n",
    "- Remover números\n",
    "- Tokenizar em palavras (importante para Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "id": "69yoqlnrr0y",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:39:43.904836Z",
     "iopub.status.busy": "2026-02-01T10:39:43.904683Z",
     "iopub.status.idle": "2026-02-01T10:39:44.594134Z",
     "shell.execute_reply": "2026-02-01T10:39:44.593679Z",
     "shell.execute_reply.started": "2026-02-01T10:39:43.904826Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:01:18.917940Z",
     "start_time": "2026-02-01T12:01:18.169215Z"
    }
   },
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Função para pré-processar texto e retornar lista de palavras (tokens)\n",
    "    \"\"\"\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover espaços extras e tokenizar\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento\n",
    "df['tokens'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de tokenização:\")\n",
    "print(f\"Original: {df['text'].iloc[0][:150]}...\")\n",
    "print(f\"\\nTokens: {df['tokens'].iloc[0][:20]}...\")\n",
    "print(f\"\\nNúmero de tokens: {len(df['tokens'].iloc[0])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de tokenização:\n",
      "Original: Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and...\n",
      "\n",
      "Tokens: ['contrary', 'to', 'other', 'reviews', 'i', 'have', 'zero', 'complaints', 'about', 'the', 'service', 'or', 'the', 'prices', 'i', 'have', 'been', 'getting', 'tire', 'service']...\n",
      "\n",
      "Número de tokens: 124\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "dsloejx50yw",
   "metadata": {},
   "source": [
    "## 5. Treinamento de Word Embeddings com Word2Vec\n",
    "\n",
    "Vamos treinar nosso próprio modelo Word2Vec no corpus de avaliações. Os parâmetros principais são:\n",
    "\n",
    "- **vector_size**: dimensão dos vetores (100 é um bom começo)\n",
    "- **window**: tamanho da janela de contexto (quantas palavras ao redor considerar)\n",
    "- **min_count**: frequência mínima de uma palavra para ser incluída no vocabulário\n",
    "- **workers**: número de threads para processamento paralelo\n",
    "- **sg**: algoritmo (0 = CBOW, 1 = Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ysokmarlbm",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:40:05.044810Z",
     "iopub.status.busy": "2026-02-01T10:40:05.044375Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:02:09.012702Z",
     "start_time": "2026-02-01T12:01:18.996065Z"
    }
   },
   "source": [
    "# Treinar modelo Word2Vec\n",
    "print(\"Treinando modelo Word2Vec...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=df['tokens'].tolist(),\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Modelo treinado com sucesso!\")\n",
    "print(f\"Tamanho do vocabulário: {len(w2v_model.wv)}\")\n",
    "print(f\"Dimensão dos vetores: {w2v_model.wv.vector_size}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com sucesso!\n",
      "Tamanho do vocabulário: 37804\n",
      "Dimensão dos vetores: 100\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "wydx4fm0och",
   "metadata": {},
   "source": [
    "### 5.1 Explorando os Embeddings Treinados\n",
    "\n",
    "Vamos testar a qualidade dos embeddings verificando palavras similares e fazendo operações vetoriais."
   ]
  },
  {
   "cell_type": "code",
   "id": "n76sr14e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:02:09.171829Z",
     "start_time": "2026-02-01T12:02:09.148495Z"
    }
   },
   "source": [
    "# Testar palavras similares\n",
    "test_words = ['good', 'bad', 'food', 'service', 'excellent', 'terrible']\n",
    "\n",
    "print(\"Palavras mais similares:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for word in test_words:\n",
    "    if word in w2v_model.wv:\n",
    "        similar = w2v_model.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\n{word.upper()}:\")\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"  - {similar_word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n{word.upper()}: não encontrada no vocabulário\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras mais similares:\n",
      "======================================================================\n",
      "\n",
      "GOOD:\n",
      "  - great: 0.8184\n",
      "  - decent: 0.8039\n",
      "  - tasty: 0.7739\n",
      "  - muchnnthe: 0.7410\n",
      "  - phenominal: 0.7377\n",
      "\n",
      "BAD:\n",
      "  - terrible: 0.7731\n",
      "  - badbad: 0.7196\n",
      "  - good: 0.7048\n",
      "  - horrible: 0.6921\n",
      "  - poor: 0.6802\n",
      "\n",
      "FOOD:\n",
      "  - foodnnthe: 0.7449\n",
      "  - belowaverage: 0.7196\n",
      "  - foodit: 0.7126\n",
      "  - foodnni: 0.7000\n",
      "  - dinnernnthe: 0.6741\n",
      "\n",
      "SERVICE:\n",
      "  - servicennwe: 0.7446\n",
      "  - servicennthe: 0.7380\n",
      "  - servicen: 0.7316\n",
      "  - serviceand: 0.7118\n",
      "  - servicennoverall: 0.6981\n",
      "\n",
      "EXCELLENT:\n",
      "  - outstanding: 0.8419\n",
      "  - fantastic: 0.8290\n",
      "  - superb: 0.7952\n",
      "  - incredible: 0.7915\n",
      "  - awesome: 0.7853\n",
      "\n",
      "TERRIBLE:\n",
      "  - horrible: 0.9302\n",
      "  - awful: 0.8524\n",
      "  - lousy: 0.7895\n",
      "  - bad: 0.7731\n",
      "  - pitiful: 0.7658\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "spuj6fzuykl",
   "metadata": {},
   "source": [
    "## 6. Criação de Vetores de Documentos\n",
    "\n",
    "Para usar embeddings de palavras em classificação de documentos, precisamos agregar os vetores das palavras em um único vetor por documento. Vamos usar a **média dos vetores** das palavras do documento.\n",
    "\n",
    "Estratégias de agregação:\n",
    "1. **Média**: somar todos os vetores e dividir pelo número de palavras\n",
    "2. **Média ponderada**: usar TF-IDF como peso\n",
    "3. **Max pooling**: pegar o máximo de cada dimensão\n",
    "4. **Doc2Vec**: treinar embeddings diretamente para documentos"
   ]
  },
  {
   "cell_type": "code",
   "id": "8dek83k05lg",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:02:12.942082Z",
     "start_time": "2026-02-01T12:02:09.172536Z"
    }
   },
   "source": [
    "def document_vector(tokens, model):\n",
    "    \"\"\"\n",
    "    Calcula o vetor de um documento como a média dos vetores de suas palavras\n",
    "    \"\"\"\n",
    "    # Filtrar palavras que estão no vocabulário\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    \n",
    "    if len(valid_tokens) == 0:\n",
    "        # Se nenhuma palavra estiver no vocabulário, retornar vetor zero\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    \n",
    "    # Calcular média dos vetores\n",
    "    vectors = [model.wv[token] for token in valid_tokens]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Criar vetores para todos os documentos\n",
    "print(\"Criando vetores de documentos...\")\n",
    "doc_vectors = np.array([document_vector(tokens, w2v_model) for tokens in df['tokens']])\n",
    "\n",
    "print(f\"Forma da matriz de features: {doc_vectors.shape}\")\n",
    "print(f\"Número de documentos: {doc_vectors.shape[0]}\")\n",
    "print(f\"Dimensão dos vetores: {doc_vectors.shape[1]}\")\n",
    "print(f\"\\nExemplo de vetor (primeiros 10 valores):\")\n",
    "print(doc_vectors[0][:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma da matriz de features: (38000, 100)\n",
      "Número de documentos: 38000\n",
      "Dimensão dos vetores: 100\n",
      "\n",
      "Exemplo de vetor (primeiros 10 valores):\n",
      "[ 0.15269439  0.05178944 -0.07896671  0.08913672 -0.09461401  0.03290601\n",
      " -0.05525561  0.27215683 -0.19804506  0.07290582]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "boykc6nsmo8",
   "metadata": {},
   "source": [
    "## 7. Divisão dos Dados em Treino e Teste\n",
    "\n",
    "Dividindo o dataset em conjuntos de treino (80%) e teste (20%) com estratificação para manter a proporção de classes."
   ]
  },
  {
   "cell_type": "code",
   "id": "dmv8fs5dquc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:02:12.975727Z",
     "start_time": "2026-02-01T12:02:12.950899Z"
    }
   },
   "source": [
    "# Preparar features e labels\n",
    "X = doc_vectors\n",
    "y = df['label']\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Tamanho do conjunto de teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"Dimensão dos vetores: {X_train.shape[1]}\")\n",
    "print(f\"\\nDistribuição de classes no treino:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nDistribuição de classes no teste:\")\n",
    "print(y_test.value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: 30400 amostras\n",
      "Tamanho do conjunto de teste: 7600 amostras\n",
      "Dimensão dos vetores: 100\n",
      "\n",
      "Distribuição de classes no treino:\n",
      "label\n",
      "2    15200\n",
      "1    15200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição de classes no teste:\n",
      "label\n",
      "1    3800\n",
      "2    3800\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "ldphlzapotg",
   "metadata": {},
   "source": [
    "## 8. Treinamento do Modelo SVM\n",
    "\n",
    "Vamos treinar um SVM com os vetores densos gerados pelos embeddings. Diferente do BoW (esparso), aqui trabalhamos com vetores densos de menor dimensionalidade."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5ijkip7jp",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:02:31.586299Z",
     "start_time": "2026-02-01T12:02:12.976590Z"
    }
   },
   "source": [
    "# Treinar modelo SVM inicial\n",
    "print(\"Treinando modelo SVM com kernel RBF...\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Modelo treinado com sucesso!\")\n",
    "print(f\"\\nNúmero de vetores de suporte: {svm_model.n_support_}\")\n",
    "print(f\"Vetores de suporte por classe: {dict(zip([1, 2], svm_model.n_support_))}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com sucesso!\n",
      "\n",
      "Número de vetores de suporte: [4640 4647]\n",
      "Vetores de suporte por classe: {1: np.int32(4640), 2: np.int32(4647)}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "bplvek3w11v",
   "metadata": {},
   "source": [
    "### 8.1 Otimização de Hiperparâmetros com GridSearchCV\n",
    "\n",
    "Vamos otimizar os hiperparâmetros do SVM para embeddings. Como os vetores são densos, kernels não-lineares (RBF) podem funcionar melhor."
   ]
  },
  {
   "cell_type": "code",
   "id": "jkj893w7zmk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:50.845200Z",
     "start_time": "2026-02-01T12:02:31.610503Z"
    }
   },
   "source": [
    "# Definir grid de hiperparâmetros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid Search com validação cruzada\n",
    "print(\"Executando Grid Search (pode levar alguns minutos)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMelhores parâmetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nMelhor score na validação cruzada: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Usar o melhor modelo\n",
    "best_svm = grid_search.best_estimator_"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores parâmetros encontrados:\n",
      "{'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "Melhor score na validação cruzada: 0.9043\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "3426dswgfe",
   "metadata": {},
   "source": [
    "## 9. Avaliação do Modelo\n",
    "\n",
    "Avaliando o desempenho do modelo otimizado no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "id": "iglybqqaggn",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:55.409477Z",
     "start_time": "2026-02-01T12:10:50.894070Z"
    }
   },
   "source": [
    "# Fazer predições no conjunto de teste\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "# Calcular acurácia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia no conjunto de teste: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Relatório de classificação completo\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negativo (1)', 'Positivo (2)']))\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(cm)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia no conjunto de teste: 0.9067 (90.67%)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negativo (1)       0.91      0.91      0.91      3800\n",
      "Positivo (2)       0.91      0.90      0.91      3800\n",
      "\n",
      "    accuracy                           0.91      7600\n",
      "   macro avg       0.91      0.91      0.91      7600\n",
      "weighted avg       0.91      0.91      0.91      7600\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[3453  347]\n",
      " [ 362 3438]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "urda3lwob1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:55.895166Z",
     "start_time": "2026-02-01T12:10:55.626206Z"
    }
   },
   "source": [
    "# Visualização da Matriz de Confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Negativo (1)', 'Positivo (2)'],\n",
    "            yticklabels=['Negativo (1)', 'Positivo (2)'])\n",
    "plt.title('Matriz de Confusão - SVM + Embeddings')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Valor Predito')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas detalhadas\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nVerdadeiros Negativos: {tn}\")\n",
    "print(f\"Falsos Positivos: {fp}\")\n",
    "print(f\"Falsos Negativos: {fn}\")\n",
    "print(f\"Verdadeiros Positivos: {tp}\")\n",
    "print(f\"\\nTaxa de Falso Positivo: {fp/(fp+tn):.4f}\")\n",
    "print(f\"Taxa de Falso Negativo: {fn/(fn+tp):.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verdadeiros Negativos: 3453\n",
      "Falsos Positivos: 347\n",
      "Falsos Negativos: 362\n",
      "Verdadeiros Positivos: 3438\n",
      "\n",
      "Taxa de Falso Positivo: 0.0913\n",
      "Taxa de Falso Negativo: 0.0953\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "coq4e9v7cy",
   "metadata": {},
   "source": [
    "## 10. Análise de Resultados e Predições\n",
    "\n",
    "Testando o modelo com exemplos novos e analisando erros."
   ]
  },
  {
   "cell_type": "code",
   "id": "leyv92ygl89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:55.941981Z",
     "start_time": "2026-02-01T12:10:55.909111Z"
    }
   },
   "source": [
    "# Função para predizer sentimento de novos textos\n",
    "def predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    Prediz o sentimento de um texto usando embeddings\n",
    "    \"\"\"\n",
    "    # Pré-processar e tokenizar\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    # Criar vetor do documento\n",
    "    doc_vec = document_vector(tokens, w2v_model).reshape(1, -1)\n",
    "    \n",
    "    # Predizer\n",
    "    prediction = best_svm.predict(doc_vec)[0]\n",
    "    \n",
    "    sentiment = \"Positivo\" if prediction == 2 else \"Negativo\"\n",
    "    return sentiment, prediction\n",
    "\n",
    "# Testar com exemplos novos\n",
    "test_reviews = [\n",
    "    \"This place is amazing! Best food I've ever had. Highly recommend!\",\n",
    "    \"Terrible service, cold food, and overpriced. Never coming back.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"Absolutely loved it! The staff was friendly and the atmosphere was great.\",\n",
    "    \"Worst experience ever. Waited for an hour and the food was disgusting.\"\n",
    "]\n",
    "\n",
    "print(\"Predições para Novos Textos:\")\n",
    "print(\"=\"*70)\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    sentiment, label = predict_sentiment(review)\n",
    "    print(f\"\\n{i}. Texto: {review}\")\n",
    "    print(f\"   Sentimento: {sentiment} (label={label})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições para Novos Textos:\n",
      "======================================================================\n",
      "\n",
      "1. Texto: This place is amazing! Best food I've ever had. Highly recommend!\n",
      "   Sentimento: Positivo (label=2)\n",
      "\n",
      "2. Texto: Terrible service, cold food, and overpriced. Never coming back.\n",
      "   Sentimento: Negativo (label=1)\n",
      "\n",
      "3. Texto: It was okay, nothing special but not bad either.\n",
      "   Sentimento: Negativo (label=1)\n",
      "\n",
      "4. Texto: Absolutely loved it! The staff was friendly and the atmosphere was great.\n",
      "   Sentimento: Positivo (label=2)\n",
      "\n",
      "5. Texto: Worst experience ever. Waited for an hour and the food was disgusting.\n",
      "   Sentimento: Negativo (label=1)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "3p9lv0y561w",
   "metadata": {},
   "source": [
    "### 10.1 Análise de Erros\n",
    "\n",
    "Examinar exemplos onde o modelo cometeu erros."
   ]
  },
  {
   "cell_type": "code",
   "id": "zs9ejy1a2kc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:55.959559Z",
     "start_time": "2026-02-01T12:10:55.944254Z"
    }
   },
   "source": [
    "# Identificar predições incorretas\n",
    "incorrect_predictions = y_test != y_pred\n",
    "incorrect_indices = y_test[incorrect_predictions].index\n",
    "\n",
    "print(f\"Total de predições incorretas: {incorrect_predictions.sum()}\")\n",
    "print(f\"Taxa de erro: {incorrect_predictions.sum() / len(y_test):.4f}\\n\")\n",
    "\n",
    "# Mostrar alguns exemplos de erros\n",
    "print(\"Exemplos de Classificações Incorretas:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_examples = min(5, len(incorrect_indices))\n",
    "for i, idx in enumerate(incorrect_indices[:num_examples], 1):\n",
    "    true_label = y_test.loc[idx]\n",
    "    pred_label = y_pred[list(y_test.index).index(idx)]\n",
    "    text = df.loc[idx, 'text']\n",
    "    \n",
    "    true_sentiment = \"Positivo\" if true_label == 2 else \"Negativo\"\n",
    "    pred_sentiment = \"Positivo\" if pred_label == 2 else \"Negativo\"\n",
    "    \n",
    "    print(f\"\\nExemplo {i}:\")\n",
    "    print(f\"Texto: {text[:200]}...\")\n",
    "    print(f\"Real: {true_sentiment} ({true_label}) | Predito: {pred_sentiment} ({pred_label})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de predições incorretas: 709\n",
      "Taxa de erro: 0.0933\n",
      "\n",
      "Exemplos de Classificações Incorretas:\n",
      "======================================================================\n",
      "\n",
      "Exemplo 1:\n",
      "Texto: Came in late on a Wednesday night about 30 min before closing. This was not my first choice as I was hoping to try some authentic French montreal food. Unfortunately most closed by 10 pm. \\n\\nIt was l...\n",
      "Real: Positivo (2) | Predito: Negativo (1)\n",
      "\n",
      "Exemplo 2:\n",
      "Texto: We really like the service and food at the bar....\n",
      "Real: Positivo (2) | Predito: Negativo (1)\n",
      "\n",
      "Exemplo 3:\n",
      "Texto: This place is all about the hotel and the shops.  It's a great place to visit and have fun.  They are more expensive than a lot of the other casinos.  The Bourbon Room (bar) is a total rip off.  Don't...\n",
      "Real: Positivo (2) | Predito: Negativo (1)\n",
      "\n",
      "Exemplo 4:\n",
      "Texto: Stayed at a hotel downtown. Went to Diesel on a Saturday night and it was an $8.00 cab ride away. Cover was $5. No coat room ladies, so dress appropriately. Diesel has good music and great VIP table r...\n",
      "Real: Negativo (1) | Predito: Positivo (2)\n",
      "\n",
      "Exemplo 5:\n",
      "Texto: Can't say I know much, but I can say it was the best dollar and two cents I have ever spent on a pretzel.\\n\\nMost of it didn't even make it out of the parking lot.\\n\\nYummmmm!...\n",
      "Real: Positivo (2) | Predito: Negativo (1)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "560pngq3x6e",
   "metadata": {},
   "source": [
    "## 11. Comparação: Embeddings vs Bag of Words\n",
    "\n",
    "Vamos comparar as principais diferenças entre as duas abordagens."
   ]
  },
  {
   "cell_type": "code",
   "id": "c1kp84x5nf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:56.261490Z",
     "start_time": "2026-02-01T12:10:55.960647Z"
    }
   },
   "source": [
    "# Criar tabela comparativa\n",
    "comparison_data = {\n",
    "    'Aspecto': [\n",
    "        'Tipo de Vetor',\n",
    "        'Dimensionalidade',\n",
    "        'Esparsidade',\n",
    "        'Captura Semântica',\n",
    "        'Contexto',\n",
    "        'Vocabulário OOV',\n",
    "        'Tempo de Treinamento',\n",
    "        'Interpretabilidade'\n",
    "    ],\n",
    "    'Bag of Words': [\n",
    "        'Esparso',\n",
    "        'Alta (5000+ features)',\n",
    "        'Muito esparso (>95% zeros)',\n",
    "        'Não',\n",
    "        'Ignora ordem e contexto',\n",
    "        'Vetor zero',\n",
    "        'Rápido (apenas contagem)',\n",
    "        'Alta (features = palavras)'\n",
    "    ],\n",
    "    'Word Embeddings': [\n",
    "        'Denso',\n",
    "        'Baixa (100-300 features)',\n",
    "        'Denso (sem zeros)',\n",
    "        'Sim',\n",
    "        'Captura contexto local',\n",
    "        'Média de palavras conhecidas',\n",
    "        'Lento (treinar Word2Vec)',\n",
    "        'Média (features abstratas)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Comparação: SVM + BoW vs SVM + Embeddings\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparação: SVM + BoW vs SVM + Embeddings\n",
      "================================================================================\n",
      "             Aspecto               Bag of Words              Word Embeddings\n",
      "       Tipo de Vetor                    Esparso                        Denso\n",
      "    Dimensionalidade      Alta (5000+ features)     Baixa (100-300 features)\n",
      "         Esparsidade Muito esparso (>95% zeros)            Denso (sem zeros)\n",
      "   Captura Semântica                        Não                          Sim\n",
      "            Contexto    Ignora ordem e contexto       Captura contexto local\n",
      "     Vocabulário OOV                 Vetor zero Média de palavras conhecidas\n",
      "Tempo de Treinamento   Rápido (apenas contagem)     Lento (treinar Word2Vec)\n",
      "  Interpretabilidade Alta (features = palavras)   Média (features abstratas)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "k993te8c7b",
   "metadata": {},
   "source": [
    "## 12. Conclusões\n",
    "\n",
    "### Resumo do Modelo\n",
    "- **Algoritmo**: Support Vector Machine (SVM) com kernel otimizado\n",
    "- **Representação**: Word2Vec embeddings (100 dimensões)\n",
    "- **Dataset**: Avaliações do Yelp com classificação binária (positivo/negativo)\n",
    "- **Agregação**: Média dos vetores de palavras por documento\n",
    "\n",
    "### Vantagens dos Embeddings\n",
    "1. **Captura semântica**: Palavras similares têm vetores próximos (ex: \"good\" e \"great\")\n",
    "2. **Menor dimensionalidade**: 100 features vs 5000+ do BoW\n",
    "3. **Vetores densos**: Toda informação é utilizada (sem esparsidade)\n",
    "4. **Generalização**: Melhor performance em palavras raras devido ao contexto\n",
    "5. **Transferência de aprendizado**: Pode usar embeddings pré-treinados (GloVe, FastText)\n",
    "\n",
    "### Desvantagens dos Embeddings\n",
    "1. **Tempo de treinamento**: Word2Vec precisa ser treinado ou carregado\n",
    "2. **Complexidade**: Mais difícil de interpretar que BoW\n",
    "3. **Agregação**: Perda de informação ao calcular média dos vetores\n",
    "4. **Dependência de corpus**: Embeddings treinados no próprio corpus podem ter vocabulário limitado\n",
    "\n",
    "### Quando usar cada abordagem?\n",
    "\n",
    "**Use BoW quando:**\n",
    "- Velocidade é crítica\n",
    "- Dataset é grande e vocabulário é rico\n",
    "- Interpretabilidade é importante\n",
    "- Recursos computacionais são limitados\n",
    "\n",
    "**Use Embeddings quando:**\n",
    "- Qualidade semântica é importante\n",
    "- Dataset é menor ou tem palavras raras\n",
    "- Quer aproveitar embeddings pré-treinados\n",
    "- Pode usar técnicas mais avançadas (CNN, LSTM)\n",
    "\n",
    "### Próximos Passos\n",
    "1. Experimentar com embeddings pré-treinados (GloVe, FastText)\n",
    "2. Testar agregações diferentes (TF-IDF weighted average, max pooling)\n",
    "3. Usar Doc2Vec para embeddings de documentos\n",
    "4. Comparar com modelos de deep learning (LSTM, Transformers)\n",
    "5. Combinar BoW e Embeddings (ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2026-02-01T12:10:56.471904Z",
     "start_time": "2026-02-01T12:10:56.426246Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
